# Memory Management

## Overview of Memory Types

### Cache
![Cache](/images/cache_levels.png)

#### L1 Cache
- Fastest and smallest memory type, usually in 16 - 64 kB range
- Separated into **instruction** (L1i) and **data** (L1d)
- grows in importance with increased speed of CPU
- *Avoids delays in data transmission and helps to make optimum use of CPU's capacity.*

#### L2 Cache
- located close to the CPU and has direct connection
- information between L2 and CPU is managed by L2 controller
- size usually < 2 MB
- Choosing between a processor with more clock speed or a larger L2 cache
    - higher clock speed => individual programs run faster
    - larger cache => several programs run simultaneously

#### L3 Cache
- Shared amongst all cores (multicore processor)
- [Cache Coherence Protocol (CCP)](https://en.wikipedia.org/wiki/Cache_coherence) can run much faster
   - Compares cache of all cores to maintain data consistency so everyone has access to all the data at the same time
- Intended to simplify and accelerate the CCP and data exchange between cores

### Temporal & Spacial Locality
Rough overview of the latency of various memory operations. Values might change based on system but the order of magnitude will be similar.
![Cache Locality](/images/cache_locality.png)
Originally from Peter Norvig: http://norvig.com/21-days.html#answers

#### Temporal Locality
- Over time the same memory address is accessed frequently (eg. in a loop)
- Keeps memory areas accessible as quickly as possible

#### Spatial Locality
- After an access to an address range, the next access to an address in the immediate vicinity is highly probable (e.g. in arrays)
- exploited by moving the adjacent address areas upwards into the next hierarchy level during a memory access.

```C++
#include <chrono>
#include <iostream>

int main()
{
    // create array
    const int size = 4;
    static int x[size][size];

    auto t1 = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < size; i++)
    {
        for (int j = 0; j < size; j++)
        {
            x[j][i] = i + j;
            std::cout << &x[j][i] << ": i=" << i << ", j=" << j << std::endl;
        }
    }

    // print execution time to console
    auto t2 = std::chrono::high_resolution_clock::now(); // stop time measurement
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(t2 - t1).count();
    std::cout << "Execution time: " << duration << " microseconds" << std::endl;

    return 0;
}
```
Code example provided by: Udacity C++ Nanodegree

#### Commands to Try
```bash
# MAC OS
sysctl -a hw

# Debian Linux
lscpu | grep Cache
```

### Virtual Memory
- Virtual memory is a very useful concept in computer architecture because it helps with making your software work well given the configuration of the respective hardware on the computer it is running on.
- In a nutshell, virtual memory guarantees us a fixed-size address space which is largely independent of the system configuration. Also, the OS guarantees that the virtual address spaces of different programs do not interfere with each other.
- The task of mapping addresses and of providing each program with its own virtual address space is performed entirely by the operating system, so from a programmer’s perspective, we usually don’t have to bother much about memory that is being used by other processes.
- Two important terms which are often used in the context of caches and virtual memory:
   - Memory Page: number of successive memory locations in a virtual memory defined by the computer architecture and the OS.
   - Memory Frame: Same as memory page but is located in the physical memory space and **NOT** the virtual memory space.
   ![Virtual Memory](/images/virtual_memory.png)
   Image by *Udacity C++ Nanodegree*

   As can be seen, both processes have their own virtual memory space. Some of the pages are mapped to frames in the physical memory and some are not. If process 1 needs to use memory in the memory page that starts at address 0x1000, a page fault will occur if the required data is not there. The memory page will then be mapped to a vacant memory frame in physical memory. Also, note that the virtual memory addresses are not the same as the physical addresses. The first memory page of process 1, which starts at the virtual address 0x0000, is mapped to a memory frame that starts at the physical address 0x2000.

   > Relocating virtual swap file to a SSD Location would help with performance.

## Variables and Memory
### The Process Memory Model
![Process Model](/images/virtual_process_model.png)
Image by *Udacity C++ Nanodegree*

We are unable to use the entire address space.
- Non-usable blocks
  - OS Kernel Space and Text are reserved for the OS
    - In **kernel space** only code trusted is executed, maintained by the OS & serves as an interface between the user code and system kernel
    - In **text space** holds code generated by the compiler and linker
- Usable blocks
  - **stack**: a contiguous memory block of fixed size
    - will crash if program exceeds size
    - used for storing automatically allocated variables such as local variables & function parameters
    - in *multithreaded programs* each thread has it's own stack memory
    - new memory is allocated when executions enters a scope and freed when scope is left
    - **STACK is *automatically* managed by the compiler i.e. no need to allocate/deallocate memory**
  - **heap**: memory space where data with dynamic storage lives
    - shared between multiple threads in a program i.e. need to take concurrency into account for memory management
    - In general memory management on the heap is (computationally) more expensive for the OS making it slower than stack memory
    - **managed by the programmer i.e. allocated memory on the heap needs to be deallocated by the programmer**
- **Block Started by Symbol (BSS)**: used in many compilers and linkers, contains global and static variables that are initialized with zero values i.e. arrays that are not initialized with predefined values
  - Memory for variables in segment is allocated once when a program is run and persists through its lifetime
- **Data**: this segments is similar to *BSS* with major difference being that variables in this segment have non-zero initialized values.
  - Memory for variables in segment is allocated once when a program is run and persists through its lifetime
> - Decision between stack and heap comes down to application. Based on the application programmer must pick the best suited space and know the advantages & disadvantages.
- By default **stack** should be the choice of memory since access is usually faster making memory management easier than the heap but has limited space and variables will only get deallocated when out of scope
- **heap** is better suited when large memory storage is required i.e. images/videos etc. However, programmer needs to carefully manage memory, if not managed correctly it can lead to memory leaks or dangling pointers.

### Memory Allocation C++
- Types of memory Allocation
  - Static Memory Allocation
    - performed for static and global variables
    - memory persists for lifetime of program
  - Automatic Memory Allocation
    - performed for functions parameters and local variables
    - stored on the **stack**
    - limited scope based on enter/exit
  - Dynamic Memory Allocation
    - way for programs to request memory from the OS at runtime as needed
    - performed on the **heap** and only limited by the size of the address space
### Automatic Memory Allocation (The Stack)
- Stack is considered *thread-safe*
### Call-By-Value vs. Call-by-reference
## Dynamic Memory Allocation (The Heap)
### Heap Memory
### Using malloc and free
### Using new and delete
### Typical Memory management problems
### Memory leaks

## Resource Copying Policies
### Copy semantics
### Lvalues and Rvalues
### Move semantics
### Using move semantics

## Smart Pointers
### Resource Acquisition is Initialization (RAII)
### Smart Pointers
### Transferring ownership
### Importance of Scope
